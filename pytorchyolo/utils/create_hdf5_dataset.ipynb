{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import h5py\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59746\n"
     ]
    }
   ],
   "source": [
    "FOLDER_PATH = \"/mnt/d/Datasets/PVDN/images/\"\n",
    "IMAGES_LIST = glob.glob(f\"{FOLDER_PATH}/*.png\")\n",
    "IMAGES_LIST.extend(glob.glob(f\"{FOLDER_PATH}/*.jpg\"))\n",
    "print(len(IMAGES_LIST))\n",
    "\n",
    "DATASET_SIZE = 59746\n",
    "# DATASET_SIZE = 30\n",
    "IMAGE_BYTES_IN_ONE_IMAGE = 600000\n",
    "\n",
    "IMAGES_PER_CHUNK = 1\n",
    "\n",
    "HDF5_PATH = \"/mnt/d/Datasets/PVDN/images_hdf5/PVDN_images_labels_map.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate HDF5 to /mnt/d/Datasets/PVDN/images_hdf5/PVDN_images_labels_map.hdf5\n",
      "Processed image 1000 to 1999\n",
      "Processed image 2000 to 2999\n",
      "Processed image 3000 to 3999\n",
      "Processed image 4000 to 4999\n",
      "Processed image 5000 to 5999\n",
      "Processed image 6000 to 6999\n",
      "Processed image 7000 to 7999\n",
      "Processed image 8000 to 8999\n",
      "Processed image 9000 to 9999\n",
      "Processed image 10000 to 10999\n",
      "Processed image 11000 to 11999\n",
      "Processed image 12000 to 12999\n",
      "Processed image 13000 to 13999\n",
      "Processed image 14000 to 14999\n",
      "Processed image 15000 to 15999\n",
      "Processed image 16000 to 16999\n",
      "Processed image 17000 to 17999\n",
      "Processed image 18000 to 18999\n",
      "Processed image 19000 to 19999\n",
      "Processed image 20000 to 20999\n",
      "Processed image 21000 to 21999\n",
      "Processed image 22000 to 22999\n",
      "Processed image 23000 to 23999\n",
      "Processed image 24000 to 24999\n",
      "Processed image 25000 to 25999\n",
      "Processed image 26000 to 26999\n",
      "Processed image 27000 to 27999\n",
      "Processed image 28000 to 28999\n",
      "Processed image 29000 to 29999\n",
      "Processed image 30000 to 30999\n",
      "Processed image 31000 to 31999\n",
      "Processed image 32000 to 32999\n",
      "Processed image 33000 to 33999\n",
      "Processed image 34000 to 34999\n",
      "Processed image 35000 to 35999\n",
      "Processed image 36000 to 36999\n",
      "Processed image 37000 to 37999\n",
      "Processed image 38000 to 38999\n",
      "Processed image 39000 to 39999\n",
      "Processed image 40000 to 40999\n",
      "Processed image 41000 to 41999\n",
      "Processed image 42000 to 42999\n",
      "Processed image 43000 to 43999\n",
      "Processed image 44000 to 44999\n",
      "Processed image 45000 to 45999\n",
      "Processed image 46000 to 46999\n",
      "Processed image 47000 to 47999\n",
      "Processed image 48000 to 48999\n",
      "Processed image 49000 to 49999\n",
      "Processed image 50000 to 50999\n",
      "Processed image 51000 to 51999\n",
      "Processed image 52000 to 52999\n",
      "Processed image 53000 to 53999\n",
      "Processed image 54000 to 54999\n",
      "Processed image 55000 to 55999\n",
      "Processed image 56000 to 56999\n",
      "Processed image 57000 to 57999\n",
      "Processed image 58000 to 58999\n",
      "Processed image 59000 to 59746\n",
      "Done processing.\n"
     ]
    }
   ],
   "source": [
    "print(\"Generate HDF5 to %s\" % HDF5_PATH)\n",
    "\n",
    "# Define the dataset dimensions and chunk sizes\n",
    "image_dataset_dims = (DATASET_SIZE, IMAGE_BYTES_IN_ONE_IMAGE)\n",
    "image_chunk_sizes = (IMAGES_PER_CHUNK, IMAGE_BYTES_IN_ONE_IMAGE)\n",
    "image_files = IMAGES_LIST\n",
    "\n",
    "with h5py.File(HDF5_PATH, \"w\") as hf:\n",
    "    pvdn_group = hf.create_group(\"PVDN\")\n",
    "    pvdn_images_ds = pvdn_group.create_dataset(\"images\", shape=image_dataset_dims, dtype=np.uint8, chunks=image_chunk_sizes, compression='lzf')\n",
    "    pvdn_labels_ds = pvdn_group.create_dataset(\"labels\", shape=(DATASET_SIZE,), dtype=h5py.special_dtype(vlen=bytes))\n",
    "    pvdn_map_ds = pvdn_group.create_dataset(\"map\", shape=(DATASET_SIZE,), dtype=h5py.string_dtype(encoding='utf-8'))\n",
    "\n",
    "\n",
    "    for i, image_file_path in enumerate(image_files):\n",
    "        if i >= DATASET_SIZE:\n",
    "            break\n",
    "\n",
    "        # Processing images\n",
    "        image = cv2.imread(str(image_file_path), cv2.IMREAD_GRAYSCALE)\n",
    "        _, image_bytes = cv2.imencode('.jpg', image)\n",
    "        image_bytes = np.resize(image_bytes, (IMAGE_BYTES_IN_ONE_IMAGE,))\n",
    "        pvdn_images_ds[i] = image_bytes\n",
    "\n",
    "        # Read the label text file\n",
    "        parent_dir = os.path.dirname(os.path.dirname(image_file_path))  # Go up two levels to reach the desired parent directory\n",
    "        label_file_path = os.path.join(parent_dir, \"labels\", os.path.splitext(os.path.basename(image_file_path))[0] + \".txt\")\n",
    "       \n",
    "\n",
    "        with open(label_file_path, \"rb\") as fileobj:\n",
    "            bytes_list = fileobj.read().splitlines()\n",
    "            # Comment line below, if other class (lines starting with 2) is required\n",
    "            bytes_list_without_other = [byte for byte in bytes_list if not byte.startswith(b'2')]\n",
    "            # print(label_file_path, filtered_bytes_no_other)\n",
    "            label_bytes = b'\\n'.join(bytes_list_without_other)\n",
    "            # print(result)\n",
    "            pvdn_labels_ds[i] = label_bytes\n",
    "        \n",
    "        image_name = os.path.splitext(os.path.basename(image_file_path))[0]\n",
    "        path = f\"{image_name}\"  # Path to the image file\n",
    "        image_index = i  # Index of the image\n",
    "        label_index = i  # Index of the label\n",
    "        data_entry = f\"{path} {image_index} {label_index}\"  # Combine the components\n",
    "        pvdn_map_ds[i] = data_entry\n",
    "        \n",
    "        \n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(f\"Processed image {i+1} to {min(i + 1000, DATASET_SIZE)}\")\n",
    "\n",
    "\n",
    "print(\"Done processing.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AptivCV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
